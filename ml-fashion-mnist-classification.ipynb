{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Classificação de artigos de moda com ML e Fashion MNIST\n\n![](https://github.com/asantos2000/ml-ipt-hw/raw/master/pics/fashion_mnist_dataset_sample.png)\n\nFigure 1. Fashion-MNIST samples (by Zalando, MIT License)\n\n## 1. Problema\n\n> Em 2021, as receitas de comércio eletrônico de varejo das vendas de vestuário e acessórios nos Estados Unidos totalizaram 180,5 bilhões de dólares, aumentando de 144,8 bilhões em 2020. - [Stadist.com](https://www.statista.com/statistics/278890/us-apparel-and-accessories-retail-e-commerce-revenue/)\n\nUm grande problema que este mercado enfrenta é categorizar essas roupas e acessórios apenas pelas imagens, especialmente quando as categorias fornecidas pelas marcas são inconsistentes.\n\nOs clientes não reconhecem as categorias dos produtos que estão buscando e desistem após a primeira pesquisa ou navegação pelas categorias. Cerca de 15% dos usuários desistem na funcionalidade de busca e 30% quando usam o seletor de categorias. Com o uso de reconhecimento por imagem, deseja-se reduzir em 10% o número de desistências em seis meses.\n\nNo armazém, estoquistas atribuem 15% das peças a categorias incorretas. Deseja-se reduzir em 10% os erros de atribuição.\n\nDeseja-se identificar, a partir de uma imagem provida pelo usuário, em um dispositivo móvel, a qual categoria aquele item pertence.\n\n### 1.1 Problema de ML\n\nO objetivo é classificar as imagens fornecidas em dez categorias (classificação múltipla) utilizando para treinamento o dataset Fashion MNist e obter uma taxa de acerto acima de 90%.\n\nOs seguintes modelos serão avaliados:\n\n- Aprendizado por transferência: Com os modelos ResNet152 V2, VGG-16, DenseNet169;\n- SVM\n- ConvNet\n\nEste trabalho está organizado da seguinte forma: 1. Discussão sobre o problema, 2. Planejamento do experimento, 3. Execução do experimento, 4. Análise e interpretação e Apresentação dos resultados.","metadata":{}},{"cell_type":"markdown","source":"## 2. Condução do experimento\n\n### 2.1 Pipeline\n\n![](https://github.com/asantos2000/ml-ipt-hw/raw/master/pics/ml-pipeline.png)","metadata":{}},{"cell_type":"markdown","source":"## Bibliotecas","metadata":{}},{"cell_type":"code","source":"#! pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:17:24.205841Z","iopub.execute_input":"2022-11-19T22:17:24.206336Z","iopub.status.idle":"2022-11-19T22:17:27.049315Z","shell.execute_reply.started":"2022-11-19T22:17:24.206299Z","shell.execute_reply":"2022-11-19T22:17:27.048106Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Only for kaggle\nfrom shutil import copyfile\n\n# upload python module as a data (file | upload data)\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"../input/module/mod_util.py\", dst = \"../working/mod_util.py\")","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:17:27.052137Z","iopub.execute_input":"2022-11-19T22:17:27.052832Z","iopub.status.idle":"2022-11-19T22:17:27.065639Z","shell.execute_reply.started":"2022-11-19T22:17:27.052788Z","shell.execute_reply":"2022-11-19T22:17:27.064331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import important libraries\nimport time\nimport wandb\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nfrom mod_util import *","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-11-19T22:17:27.067474Z","iopub.execute_input":"2022-11-19T22:17:27.068177Z","iopub.status.idle":"2022-11-19T22:17:40.135315Z","shell.execute_reply.started":"2022-11-19T22:17:27.068138Z","shell.execute_reply":"2022-11-19T22:17:40.134107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Globais\ndf_model_metrics = pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:17:40.138317Z","iopub.execute_input":"2022-11-19T22:17:40.139368Z","iopub.status.idle":"2022-11-19T22:17:40.150728Z","shell.execute_reply.started":"2022-11-19T22:17:40.139328Z","shell.execute_reply":"2022-11-19T22:17:40.148637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Coleta de métrias de hardware\nos.environ[\"WANDB_NOTEBOOK_NAME\"] = \"ml-fashion-mnist-classification.ipynb\"\n#os.environ[\"WANDB_API_KEY\"] = \"key\"\nwandb.init(project=\"transfer-learning\")","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:17:40.152303Z","iopub.execute_input":"2022-11-19T22:17:40.152589Z","iopub.status.idle":"2022-11-19T22:17:59.800235Z","shell.execute_reply.started":"2022-11-19T22:17:40.152563Z","shell.execute_reply":"2022-11-19T22:17:59.799199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Descrição do conjunto de dados\n\nFashion-MNIST é um conjunto de dados de imagens de artigos de Zalando—consistindo em um conjunto de treinamento de 60.000 exemplos e um conjunto de teste de 10.000 amostras. Cada amostra é uma imagem em tons de cinza 28x28, associada a um rótulo de 10 classes. Zalando pretende que o Fashion-MNIST sirva como um substituto direto para o conjunto de dados MNIST original para benchmarking de algoritmos de aprendizado de máquina.\n\nEle compartilha o tamanho exato da imagem e a estrutura das divisões de treinamento e teste. O conjunto de dados MNIST original contém muitos dígitos manuscritos. Os membros da comunidade AI/ML/Data Science adoram esse conjunto de dados e o usam como referência para validar seus algoritmos. Na verdade, o MNIST é frequentemente o primeiro conjunto de dados que os pesquisadores tentam.\n\nCada imagem tem 28 pixels de altura e 28 pixels de largura, para um total de 784 pixels no total.\nCada pixel tem um único valor de pixel associado a ele, indicando a claridade ou escuridão daquele pixel, com números mais altos significando mais escuro. Este valor de pixel é um número inteiro entre 0 e 255.\n\nOs conjuntos de dados de treinamento e teste têm 785 colunas. A primeira coluna é composta pelos rótulos das classes (veja acima), e representa a peça de vestuário. O restante das colunas contém os valores de pixel da imagem associada.\n\nPara localizar um pixel na imagem, suponha que decompusemos x como x = i * 28 + j, onde i e j são inteiros entre 0 e 27. O pixel está localizado na linha i e coluna j de uma matriz 28 x 28 . Por exemplo, pixel31 indica o pixel que está na quarta coluna da esquerda e na segunda linha da parte superior, como no diagrama ascii abaixo.","metadata":{}},{"cell_type":"markdown","source":"## 3. Execução do experimento\n\nA execução do experimento está dividida em etapas:\n\n3.1 Preparar o conjunto de dados padrão;\n3.2 Engenharia de requisitos;\n3.3 Selecionar e treinar os modelos;\n3.4 Avaliar os modelos;\n3.5 Ajustar os modelos.\n\n### 3.1 Preparando o conjunto de dados\n\nOs modelos SVM e ConvNet não necessitam de modificações nos dados, podendo ser treinado com as imagens de tamanho 28x28 e um canal, porém, para transferência de conhecimento é necessário ajustá-las para no mínimo 48x48 com três canais (RGB).\n\n#### 3.1.1 Coletando e rotulando os dados\n\nO conjunto de dados Fashion do MNist já está rotulado e faz parte da biblioteca de conjuntos de dados do Keras.","metadata":{}},{"cell_type":"markdown","source":"Formato dos conjuntos de dados:","metadata":{}},{"cell_type":"code","source":"train_X, train_Y, test_X, test_Y = load_mnist_dataset()\n\ntrain_X.shape, train_Y.shape, test_X.shape, test_Y.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:17:59.804793Z","iopub.execute_input":"2022-11-19T22:17:59.807261Z","iopub.status.idle":"2022-11-19T22:18:01.107409Z","shell.execute_reply.started":"2022-11-19T22:17:59.807217Z","shell.execute_reply":"2022-11-19T22:18:01.106307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ext_X, test_ext_Y = load_extra_dataset(\"../input/extra-fashion-test/test_images\")\n\ntest_ext_X.shape, test_ext_Y.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:18:01.112005Z","iopub.execute_input":"2022-11-19T22:18:01.114957Z","iopub.status.idle":"2022-11-19T22:18:03.781340Z","shell.execute_reply.started":"2022-11-19T22:18:01.114915Z","shell.execute_reply":"2022-11-19T22:18:03.780042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1.2 Avaliando os dados","metadata":{}},{"cell_type":"code","source":"dataset_size = train_X.shape[0]+test_X.shape[0]\ntrain_size = train_X.shape[0]\ntest_size = test_X.shape[0]\nextra_test_size = test_ext_X.shape[0]\n\nprint(f\"Train with {train_size:,} images that represents {round(train_size / dataset_size*100,2)}% of dataset size of {dataset_size:,}.\")\nprint(f\"Test with {test_size:,} images that represents {round(test_size / dataset_size*100,2)}% of dataset size of {dataset_size:,}.\")\nprint(f\"Test with extra {extra_test_size:,} images that represents {round(extra_test_size / dataset_size*100,2)}% of dataset size of {dataset_size:,}.\")","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:18:03.785901Z","iopub.execute_input":"2022-11-19T22:18:03.788990Z","iopub.status.idle":"2022-11-19T22:18:03.803347Z","shell.execute_reply.started":"2022-11-19T22:18:03.788948Z","shell.execute_reply":"2022-11-19T22:18:03.801988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribuição dos conjuntos de dados:","metadata":{}},{"cell_type":"code","source":"train_data = pd.DataFrame(np.asarray(np.c_[train_Y, train_X.reshape(train_X.shape[0], 784)]))\ntest_data = pd.DataFrame(np.asarray(np.c_[test_Y, test_X.reshape(test_X.shape[0], 784)]))\ntest_ext_data = pd.DataFrame(np.asarray(np.c_[test_ext_Y, test_ext_X.reshape(test_ext_X.shape[0], 784)]))\n\nprint(\"--- Train data ---\")\nget_classes_distribution(train_data)\nprint(\"--- Test data ---\")\nget_classes_distribution(test_data)\nprint(\"--- Test with extra data ---\")\nget_classes_distribution(test_ext_data)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:18:03.808543Z","iopub.execute_input":"2022-11-19T22:18:03.809172Z","iopub.status.idle":"2022-11-19T22:18:03.875390Z","shell.execute_reply.started":"2022-11-19T22:18:03.809127Z","shell.execute_reply":"2022-11-19T22:18:03.874319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Engenharia de características \n\nPara o treinamento do modelo SVM foi aplicada a redução do número de caracteristicas utilizando HOG, já para os modelos pré treinados foi necessário aumentar esse conjunto, como demonstrado nas seções seguintes.\n\n### 3.2.1 Preprocessar os dados\n\nPara aplicar a transferência de conhecimento (_transfer learning_) para os modelos selecionado, é necessário:\n\n- Converter as imagens em 3 canais para ajustá-las a entrada dos modelos;\n- Redefinir o formato para o formato do _tensor_ (requerido pelo tensorflow);\n- Redimensionar as imagens para 48x48.\n","metadata":{}},{"cell_type":"code","source":"train_X = adjust_data_for_transfer_learning(train_X, 48)\ntest_X = adjust_data_for_transfer_learning(test_X, 48)\ntest_ext_X = adjust_data_for_transfer_learning(test_ext_X, 48)\nprint(f\"train_X: {train_X.shape}\")\nprint(f\"test_X: {test_X.shape}\")\nprint(f\"test_ext_X: {test_ext_X.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:18:03.883602Z","iopub.execute_input":"2022-11-19T22:18:03.886341Z","iopub.status.idle":"2022-11-19T22:18:07.780221Z","shell.execute_reply.started":"2022-11-19T22:18:03.886302Z","shell.execute_reply":"2022-11-19T22:18:07.778871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Formato e exemplo de uma figura no conjunto de dados.","metadata":{}},{"cell_type":"code","source":"exibe_bitmap_primeira_imagem(train_X)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:18:07.782019Z","iopub.execute_input":"2022-11-19T22:18:07.782391Z","iopub.status.idle":"2022-11-19T22:18:08.416027Z","shell.execute_reply.started":"2022-11-19T22:18:07.782353Z","shell.execute_reply":"2022-11-19T22:18:08.415120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.2 Normalizar os dados\n\nOs dados devem ser pré-processados antes de treinar a rede. Ao inspecionar a primeira imagem no conjunto de treinamento, verá que os valores de pixel ficam no intervalo de 0 a 255.\n\nEscalamos esses valores para um intervalo de 0 a 1 antes de alimentá-los ao modelo de rede neural. Para isso, dividimos os valores por 255. É importante que o conjunto de treinamento e o conjunto de teste sejam pré-processados da mesma forma.","metadata":{}},{"cell_type":"code","source":"# Normalize the data and change data type\ntrain_X = train_X / 255.\ntest_X = test_X / 255.\ntest_ext_X = test_ext_X / 255.","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:18:08.417105Z","iopub.execute_input":"2022-11-19T22:18:08.417463Z","iopub.status.idle":"2022-11-19T22:18:09.944835Z","shell.execute_reply.started":"2022-11-19T22:18:08.417429Z","shell.execute_reply":"2022-11-19T22:18:09.943626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conjunto de dados normalizado:","metadata":{}},{"cell_type":"code","source":"exibe_grade_imagens(AMOSTRAS_GRID, train_X, train_Y, must_reshape=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:18:09.950657Z","iopub.execute_input":"2022-11-19T22:18:09.953856Z","iopub.status.idle":"2022-11-19T22:18:12.373991Z","shell.execute_reply.started":"2022-11-19T22:18:09.953810Z","shell.execute_reply":"2022-11-19T22:18:12.373109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conjunto extra de dados normalizado.","metadata":{}},{"cell_type":"code","source":"exibe_grade_imagens(AMOSTRAS_GRID, test_ext_X, test_ext_Y, must_reshape=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:18:12.375129Z","iopub.execute_input":"2022-11-19T22:18:12.375487Z","iopub.status.idle":"2022-11-19T22:18:14.353396Z","shell.execute_reply.started":"2022-11-19T22:18:12.375455Z","shell.execute_reply":"2022-11-19T22:18:14.352329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.3 Converter rótulos em codificador one-hot (para categórico)\n\nPara variáveis categóricas em que não existe relacionamento ordinal, a codificação inteira pode não ser suficiente, na melhor das hipóteses, ou enganosa para o modelo, na pior.\n\nForçar uma relação ordinal por meio de uma codificação ordinal e permitir que o modelo assuma uma ordenação natural entre categorias pode resultar em desempenho ruim ou resultados inesperados.\n\nNesse caso, uma codificação _one-hot_ pode ser aplicada à representação ordinal. É aqui que a variável codificada de inteiro é removida e uma nova variável binária é adicionada para cada valor inteiro exclusivo na variável.\n\n> Cada bit representa uma categoria possível. Se a variável não pode pertencer a várias categorias ao mesmo tempo, apenas um bit no grupo pode estar “ligado”. Isso é chamado de codificação one-hot.","metadata":{}},{"cell_type":"code","source":"train_Y_one_hot = to_categorical(train_Y)\ntest_Y_one_hot = to_categorical(test_Y)\ntest_ext_Y_one_hot = to_categorical(test_ext_Y) ","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:18:14.354832Z","iopub.execute_input":"2022-11-19T22:18:14.359312Z","iopub.status.idle":"2022-11-19T22:18:14.369628Z","shell.execute_reply.started":"2022-11-19T22:18:14.359272Z","shell.execute_reply":"2022-11-19T22:18:14.368596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Examples:\")\nprint(f\"Category: {train_Y[0]}, Dummy vars: {train_Y_one_hot[0]}\")\nprint(f\"Category: {train_Y[1]}, Dummy vars: {train_Y_one_hot[1]}\")","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:18:14.371081Z","iopub.execute_input":"2022-11-19T22:18:14.372639Z","iopub.status.idle":"2022-11-19T22:18:14.384344Z","shell.execute_reply.started":"2022-11-19T22:18:14.372591Z","shell.execute_reply":"2022-11-19T22:18:14.382928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.4 Dividindo os dados de treinamento em treinamento e validação","metadata":{}},{"cell_type":"code","source":"train_X, valid_X, train_label, valid_label = train_test_split(train_X,\n                                                              train_Y_one_hot,\n                                                              test_size=0.05,\n                                                              random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:18:14.385891Z","iopub.execute_input":"2022-11-19T22:18:14.387035Z","iopub.status.idle":"2022-11-19T22:18:15.504388Z","shell.execute_reply.started":"2022-11-19T22:18:14.386994Z","shell.execute_reply":"2022-11-19T22:18:15.503251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Selecionar e treinar os modelos\n\nPara transferência de aprendizado, foram escolhidos arbitrariamente três modelos com arquiteturas distintas do módulo de aplicativos do Keras. Eles são modelos de aprendizado profundo que são disponibilizados juntamente com pesos pré-treinados. Esses modelos podem ser usados para previsão, extração de recursos e ajuste fino.\n\nCom base na avaliação do keras dos [modelo disponíveis](https://keras.io/api/applications/), selecionamos modelos com acurária (Top-5) acima de 90%.\n\nEsses modelos são:\n\n1. [DenseNet169](https://keras.io/api/applications/densenet/#densenet169-function), uma rede convolucionais densamente conectadas (HUANG et al., 2017)\n2. [ResNet152V2](https://keras.io/api/applications/resnet/#resnet152v2-function), uma redes residuais profundas. (RE et al., 2016)\n3. e a [VGG-16](https://keras.io/api/applications/vgg/#vgg16-function), uma dede convolucional muito profundas para reconhecimento de imagem em grande escala (SIMONYAN et al., 2014)\n\nDe acordo com essa [avaliação](https://keras.io/api/applications/), essas redes tem o seguinte desempenho:\n\n| Modelo      | Tamanho (MB) | Top-1 Acurácia | Top-5 Acurácia | Paâmetros  | Profundidade |\n| ---         | ---          | ---            | ---            | ---        | ---          |\n| DenseNet169 | 57           | 76.2%          | 93.2%          | 14.3M      | 338          |\n| ResNet152V2 | 232          | 78.0%          | 94.2%          | 60.4M      | 307          |\n| VGG16       | 528          | 71.3%          | 90.1%          | 138.4M     | 16           |\n\n> A profundidade conta o número de camadas com parâmetros.\n\nO DenseNet-169 foi escolhido porque, apesar de ter uma profundidade de 169 camadas, é relativamente baixo em parâmetros em comparação com outros modelos, e a arquitetura lida bem com o problema do gradiente de fuga.\n\nA escolha da ResNet é por causa da sua arquitetura, ela aprende com as funções residuais em vez de aprender com o sinal diretamente.\n\nA VGG16 é a escolha preferida da comunidade para extrair recursos de imagens. A configuração de peso do VGGNet está disponível publicamente e tem sido usada em muitos outros aplicativos e desafios como um extrator de recursos de linha de base. (SIMONYAN et al., 2014)","metadata":{}},{"cell_type":"markdown","source":"### 3.3.1 Definindo os modelos para tranferência de conhecimento (Pre-training models)\n\n","metadata":{}},{"cell_type":"markdown","source":"Hiperparametros","metadata":{}},{"cell_type":"code","source":"image_size = train_X[0].shape[0]\nchannels = 3\nprint(\"imageSize: \", image_size)\n\nepochs = 30\nbatch_size = 700","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:18:15.505942Z","iopub.execute_input":"2022-11-19T22:18:15.506663Z","iopub.status.idle":"2022-11-19T22:18:15.515443Z","shell.execute_reply.started":"2022-11-19T22:18:15.506624Z","shell.execute_reply":"2022-11-19T22:18:15.514344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 Treinar modelo","metadata":{}},{"cell_type":"markdown","source":"### DenseNet169","metadata":{}},{"cell_type":"code","source":"model_DenseNet169 = adj_model_DenseNet169(image_size, channels)\n\n# Train\nstart = time.perf_counter()\nhistory_DenseNet169_model = model_DenseNet169.fit(train_X, \n                                                train_label, \n                                                validation_data = (valid_X,\n                                                                   valid_label), \n                                                epochs = epochs, \n                                                batch_size = batch_size, \n                                                verbose = 1)\nend = time.perf_counter()\ntrain_duration_DenseNet169 = end - start","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:18:15.517278Z","iopub.execute_input":"2022-11-19T22:18:15.523641Z","iopub.status.idle":"2022-11-19T22:30:09.856939Z","shell.execute_reply.started":"2022-11-19T22:18:15.523600Z","shell.execute_reply":"2022-11-19T22:30:09.855813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualizando o modelo:","metadata":{}},{"cell_type":"code","source":"plot_model(model_DenseNet169, \n           to_file = \"plot-densenet169.png\", \n           show_shapes = True, \n           show_layer_names = True)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:30:09.858691Z","iopub.execute_input":"2022-11-19T22:30:09.859048Z","iopub.status.idle":"2022-11-19T22:30:15.975618Z","shell.execute_reply.started":"2022-11-19T22:30:09.859014Z","shell.execute_reply":"2022-11-19T22:30:15.973840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ResNet152V2","metadata":{}},{"cell_type":"code","source":"model_ResNet152V2 = adj_model_ResNet152V2(image_size, channels)\n\n# Train\nstart = time.perf_counter()\nhistory_ResNet152V2_model = model_ResNet152V2.fit(train_X, \n                                                train_label, \n                                                validation_data = (valid_X,\n                                                                    valid_label), \n                                                epochs = epochs, \n                                                batch_size = batch_size, \n                                                verbose = 1)\nend = time.perf_counter()\ntrain_duration_ResNet152V2 = end - start","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:30:15.978282Z","iopub.execute_input":"2022-11-19T22:30:15.979110Z","iopub.status.idle":"2022-11-19T22:51:55.724394Z","shell.execute_reply.started":"2022-11-19T22:30:15.979037Z","shell.execute_reply":"2022-11-19T22:51:55.723454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model_ResNet152V2,\n           to_file = \"plot-resnet152v2.png\", \n           show_shapes = True, \n           show_layer_names = True)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:51:55.732287Z","iopub.execute_input":"2022-11-19T22:51:55.732749Z","iopub.status.idle":"2022-11-19T22:51:59.874705Z","shell.execute_reply.started":"2022-11-19T22:51:55.732707Z","shell.execute_reply":"2022-11-19T22:51:59.873169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### VGG-16","metadata":{}},{"cell_type":"code","source":"model_VGG16 = adj_model_VGG16(image_size, channels)\n\n# Train\nstart = time.perf_counter()\nhistory_VGG16_model = model_VGG16.fit(train_X, \n                                      train_label, \n                                      validation_data = (valid_X,\n                                                        valid_label), \n                                      epochs = epochs,\n                                      batch_size = batch_size, \n                                      verbose = 1)\nend = time.perf_counter()\ntrain_duration_VGG16 = end - start","metadata":{"execution":{"iopub.status.busy":"2022-11-19T22:51:59.877679Z","iopub.execute_input":"2022-11-19T22:51:59.878394Z","iopub.status.idle":"2022-11-19T23:01:14.977965Z","shell.execute_reply.started":"2022-11-19T22:51:59.878354Z","shell.execute_reply":"2022-11-19T23:01:14.976797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model_VGG16,\n           to_file = \"plot-vgg16.png\",\n           show_shapes = True,\n           show_layer_names = True)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:01:14.987198Z","iopub.execute_input":"2022-11-19T23:01:14.987484Z","iopub.status.idle":"2022-11-19T23:01:15.988265Z","shell.execute_reply.started":"2022-11-19T23:01:14.987457Z","shell.execute_reply":"2022-11-19T23:01:15.987072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 Avaliar os modelos\n\nA métrica de avaliação para os modelos será a precisão multiclasse.\n\n### 3.4.1 Visualizando a acurácia e as perdas ","metadata":{}},{"cell_type":"markdown","source":"#### DenseNet169","metadata":{}},{"cell_type":"code","source":"plot_acc_loss(history_DenseNet169_model, \"DenseNet169\", epochs);","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:01:15.990566Z","iopub.execute_input":"2022-11-19T23:01:15.991242Z","iopub.status.idle":"2022-11-19T23:01:18.072282Z","shell.execute_reply.started":"2022-11-19T23:01:15.991199Z","shell.execute_reply":"2022-11-19T23:01:18.071218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ResNet152V2","metadata":{}},{"cell_type":"code","source":"plot_acc_loss(history_ResNet152V2_model, \"ResNet152V2\", epochs);","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:01:18.074028Z","iopub.execute_input":"2022-11-19T23:01:18.074422Z","iopub.status.idle":"2022-11-19T23:01:18.142597Z","shell.execute_reply.started":"2022-11-19T23:01:18.074387Z","shell.execute_reply":"2022-11-19T23:01:18.141737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### VGG16","metadata":{}},{"cell_type":"code","source":"plot_acc_loss(history_VGG16_model, \"VGG16\", epochs);","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:01:18.143861Z","iopub.execute_input":"2022-11-19T23:01:18.144193Z","iopub.status.idle":"2022-11-19T23:01:18.226438Z","shell.execute_reply.started":"2022-11-19T23:01:18.144157Z","shell.execute_reply":"2022-11-19T23:01:18.225462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.4.2 Testando os modelos","metadata":{}},{"cell_type":"markdown","source":"#### DenseNet169","metadata":{}},{"cell_type":"code","source":"print(\"Avaliando DenseNet169\")\nprint(model_DenseNet169.metrics_names)\nme = model_DenseNet169.evaluate(test_X, test_Y_one_hot)\nprint(me)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:01:18.233622Z","iopub.execute_input":"2022-11-19T23:01:18.234237Z","iopub.status.idle":"2022-11-19T23:01:39.987498Z","shell.execute_reply.started":"2022-11-19T23:01:18.234198Z","shell.execute_reply":"2022-11-19T23:01:39.986367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Avaliando DenseNet169\")\nprint(model_DenseNet169.metrics_names)\nme =model_DenseNet169.evaluate(test_ext_X, test_ext_Y_one_hot)\nprint(me)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:01:39.989242Z","iopub.execute_input":"2022-11-19T23:01:39.989586Z","iopub.status.idle":"2022-11-19T23:01:40.492493Z","shell.execute_reply.started":"2022-11-19T23:01:39.989550Z","shell.execute_reply":"2022-11-19T23:01:40.491549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" #### ResNet152V2","metadata":{}},{"cell_type":"code","source":"print(\"Avaliando ResNet152V2\")\nprint(model_ResNet152V2.metrics_names)\nme = model_ResNet152V2.evaluate(test_X, test_Y_one_hot)\nprint(me)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:01:40.494028Z","iopub.execute_input":"2022-11-19T23:01:40.494418Z","iopub.status.idle":"2022-11-19T23:01:54.927123Z","shell.execute_reply.started":"2022-11-19T23:01:40.494382Z","shell.execute_reply":"2022-11-19T23:01:54.925589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Avaliando ResNet152V2\")\nprint(model_ResNet152V2.metrics_names)\nme = model_ResNet152V2.evaluate(test_ext_X, test_ext_Y_one_hot)\nprint(me)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:01:54.928734Z","iopub.execute_input":"2022-11-19T23:01:54.929115Z","iopub.status.idle":"2022-11-19T23:01:55.644239Z","shell.execute_reply.started":"2022-11-19T23:01:54.929079Z","shell.execute_reply":"2022-11-19T23:01:55.643084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### VGG16","metadata":{}},{"cell_type":"code","source":"print(\"Avaliando VGG-16\")\nprint(model_VGG16.metrics_names)\nme = model_VGG16.evaluate(test_X, test_Y_one_hot)\nprint(me)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:01:55.649197Z","iopub.execute_input":"2022-11-19T23:01:55.649564Z","iopub.status.idle":"2022-11-19T23:02:01.599176Z","shell.execute_reply.started":"2022-11-19T23:01:55.649529Z","shell.execute_reply":"2022-11-19T23:02:01.598228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Avaliando VGG-16\")\nprint(model_VGG16.metrics_names)\nme = model_VGG16.evaluate(test_ext_X, test_ext_Y_one_hot)\nprint(me)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:02:01.600832Z","iopub.execute_input":"2022-11-19T23:02:01.601326Z","iopub.status.idle":"2022-11-19T23:02:01.975401Z","shell.execute_reply.started":"2022-11-19T23:02:01.601287Z","shell.execute_reply":"2022-11-19T23:02:01.974126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.4.3 Predições com os modelos\n\nAvaliação das previsões em relação ao _Ground Truth_.","metadata":{}},{"cell_type":"code","source":"# predict DenseNet169 Model\nstart = time.perf_counter()\npred_Y_DenseNet169 = model_DenseNet169.predict(test_X)\nend = time.perf_counter()\n\nshow_predict(pred_Y_DenseNet169, test_X, test_Y, \"DenseNet169\")\n\npredict_duration_DenseNet169 =  end - start","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:02:01.976985Z","iopub.execute_input":"2022-11-19T23:02:01.977628Z","iopub.status.idle":"2022-11-19T23:02:27.490151Z","shell.execute_reply.started":"2022-11-19T23:02:01.977589Z","shell.execute_reply":"2022-11-19T23:02:27.489283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict ResNet152V2 Model\nstart = time.perf_counter()\npred_Y_ResNet152V2 = model_ResNet152V2.predict(test_X)\nend = time.perf_counter()\n\nshow_predict(pred_Y_ResNet152V2, test_X, test_Y, \"ResNet152V2\")\n\npredict_duration_ResNet152V2 = predict_duration_DenseNet169","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:02:27.491693Z","iopub.execute_input":"2022-11-19T23:02:27.492269Z","iopub.status.idle":"2022-11-19T23:02:42.510463Z","shell.execute_reply.started":"2022-11-19T23:02:27.492226Z","shell.execute_reply":"2022-11-19T23:02:42.509506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict VGG16 Model\nstart = time.perf_counter()\npred_Y_VGG16 = model_VGG16.predict(test_X)\nend = time.perf_counter()\n\nshow_predict(pred_Y_VGG16, test_X, test_Y, \"VGG16\")\n\npredict_duration_VGG16 = end - start","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:02:42.511904Z","iopub.execute_input":"2022-11-19T23:02:42.512898Z","iopub.status.idle":"2022-11-19T23:02:48.180736Z","shell.execute_reply.started":"2022-11-19T23:02:42.512858Z","shell.execute_reply":"2022-11-19T23:02:48.179745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predição com conjunto extra de dados","metadata":{}},{"cell_type":"code","source":"#predict DenseNet169 Model\npred_ext_Y_DenseNet169 = model_DenseNet169.predict(test_ext_X)\nshow_predict(pred_Y_DenseNet169, test_ext_X, test_ext_Y, \"DenseNet169\")","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:02:48.182329Z","iopub.execute_input":"2022-11-19T23:02:48.182970Z","iopub.status.idle":"2022-11-19T23:02:49.707836Z","shell.execute_reply.started":"2022-11-19T23:02:48.182932Z","shell.execute_reply":"2022-11-19T23:02:49.706722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predict ResNet152V2 Model\npred_ext_Y_ResNet152V2 = model_ResNet152V2.predict(test_ext_X)\nshow_predict(pred_Y_ResNet152V2, test_ext_X, test_ext_Y, \"ResNet152V2\")","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:02:49.709719Z","iopub.execute_input":"2022-11-19T23:02:49.712086Z","iopub.status.idle":"2022-11-19T23:02:52.030740Z","shell.execute_reply.started":"2022-11-19T23:02:49.712037Z","shell.execute_reply":"2022-11-19T23:02:52.029728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predict VGG16 Model\npred_ext_Y_VGG16 = model_VGG16.predict(test_ext_X)\nshow_predict(pred_Y_VGG16, test_ext_X, test_ext_Y, \"VGG16\")","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:02:52.032385Z","iopub.execute_input":"2022-11-19T23:02:52.033530Z","iopub.status.idle":"2022-11-19T23:02:53.521413Z","shell.execute_reply.started":"2022-11-19T23:02:52.033487Z","shell.execute_reply":"2022-11-19T23:02:53.520399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.4.4 Matriz de confusão para verificar a precisão","metadata":{}},{"cell_type":"code","source":"# confusion matrix for DenseNet169 Model\nshow_confusion_matrix(test_Y, pred_Y_DenseNet169)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:02:53.533017Z","iopub.execute_input":"2022-11-19T23:02:53.533617Z","iopub.status.idle":"2022-11-19T23:02:54.119328Z","shell.execute_reply.started":"2022-11-19T23:02:53.533581Z","shell.execute_reply":"2022-11-19T23:02:54.118293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix for ResNet152V2 Model\nshow_confusion_matrix(test_Y, pred_Y_ResNet152V2)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:04:08.510295Z","iopub.execute_input":"2022-11-19T23:04:08.510722Z","iopub.status.idle":"2022-11-19T23:04:09.086797Z","shell.execute_reply.started":"2022-11-19T23:04:08.510676Z","shell.execute_reply":"2022-11-19T23:04:09.085597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix for VGG16 Model\nshow_confusion_matrix(test_Y, pred_Y_VGG16)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:04:15.614653Z","iopub.execute_input":"2022-11-19T23:04:15.615042Z","iopub.status.idle":"2022-11-19T23:04:16.179123Z","shell.execute_reply.started":"2022-11-19T23:04:15.615011Z","shell.execute_reply":"2022-11-19T23:04:16.178099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Matriz de confusão com conjunto extra de dados.","metadata":{}},{"cell_type":"code","source":"# confusion matrix for DenseNet169 Model\nshow_confusion_matrix(test_ext_Y, pred_ext_Y_DenseNet169)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:04:26.129174Z","iopub.execute_input":"2022-11-19T23:04:26.129542Z","iopub.status.idle":"2022-11-19T23:04:26.685240Z","shell.execute_reply.started":"2022-11-19T23:04:26.129511Z","shell.execute_reply":"2022-11-19T23:04:26.684324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix for ResNet152V2 Model\nshow_confusion_matrix(test_ext_Y, pred_ext_Y_ResNet152V2)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:04:34.331134Z","iopub.execute_input":"2022-11-19T23:04:34.331634Z","iopub.status.idle":"2022-11-19T23:04:35.019262Z","shell.execute_reply.started":"2022-11-19T23:04:34.331596Z","shell.execute_reply":"2022-11-19T23:04:35.018006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix for VGG16 Model\nshow_confusion_matrix(test_ext_Y, pred_ext_Y_VGG16)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:04:41.849750Z","iopub.execute_input":"2022-11-19T23:04:41.850178Z","iopub.status.idle":"2022-11-19T23:04:42.417132Z","shell.execute_reply.started":"2022-11-19T23:04:41.850141Z","shell.execute_reply":"2022-11-19T23:04:42.416124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.4.5 Relatório de classificação","metadata":{}},{"cell_type":"code","source":"# Classification Report for DenseNet169 Model\ncr = show_classification_report(test_Y, pred_Y_DenseNet169, CLASS_NAMES, \"DenseNet169\")\n\n# Prepare report\ndf_model_metrics = df_model_metrics.append(add_model_metrics(cr, train_duration_DenseNet169, predict_duration_DenseNet169, \"DenseNet169\", 0))","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:04:48.875386Z","iopub.execute_input":"2022-11-19T23:04:48.875772Z","iopub.status.idle":"2022-11-19T23:04:48.973398Z","shell.execute_reply.started":"2022-11-19T23:04:48.875741Z","shell.execute_reply":"2022-11-19T23:04:48.972177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report for ResNet152V2 Model\ncr = show_classification_report(test_Y, pred_Y_ResNet152V2, CLASS_NAMES, \"ResNet152V2\")\n\n# Prepare report\ndf_model_metrics = df_model_metrics.append(add_model_metrics(cr, train_duration_ResNet152V2, predict_duration_ResNet152V2, \"ResNet152V2\", 1))","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:05:15.250762Z","iopub.execute_input":"2022-11-19T23:05:15.251208Z","iopub.status.idle":"2022-11-19T23:05:15.326219Z","shell.execute_reply.started":"2022-11-19T23:05:15.251173Z","shell.execute_reply":"2022-11-19T23:05:15.324991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report for DenseNet169 Model\ncr = show_classification_report(test_Y, pred_Y_VGG16, CLASS_NAMES, \"VGG16\")\n\n# Prepare report\ndf_model_metrics = df_model_metrics.append(add_model_metrics(cr, train_duration_VGG16, predict_duration_VGG16, \"VGG16\", 2))","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:05:19.304377Z","iopub.execute_input":"2022-11-19T23:05:19.304762Z","iopub.status.idle":"2022-11-19T23:05:19.369364Z","shell.execute_reply.started":"2022-11-19T23:05:19.304730Z","shell.execute_reply":"2022-11-19T23:05:19.368104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Relatório de classificação com conjunto extra de dados.","metadata":{}},{"cell_type":"code","source":"# Classification Report for DenseNet169 Model\ncr = show_classification_report(test_ext_Y, pred_ext_Y_DenseNet169, CLASS_NAMES, \"DenseNet169\")","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:06:30.045892Z","iopub.execute_input":"2022-11-19T23:06:30.046270Z","iopub.status.idle":"2022-11-19T23:06:30.070729Z","shell.execute_reply.started":"2022-11-19T23:06:30.046238Z","shell.execute_reply":"2022-11-19T23:06:30.069763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report for ResNet152V2 Model\ncr = show_classification_report(test_ext_Y, pred_ext_Y_ResNet152V2, CLASS_NAMES, \"ResNet152V2\")","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:06:36.940246Z","iopub.execute_input":"2022-11-19T23:06:36.940656Z","iopub.status.idle":"2022-11-19T23:06:36.967084Z","shell.execute_reply.started":"2022-11-19T23:06:36.940615Z","shell.execute_reply":"2022-11-19T23:06:36.966111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report for DenseNet169 Model\ncr = show_classification_report(test_ext_Y, pred_ext_Y_VGG16, CLASS_NAMES, \"VGG16\")","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:06:43.761703Z","iopub.execute_input":"2022-11-19T23:06:43.762104Z","iopub.status.idle":"2022-11-19T23:06:43.789025Z","shell.execute_reply.started":"2022-11-19T23:06:43.762033Z","shell.execute_reply":"2022-11-19T23:06:43.787775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusão\n\nOs três modelos obtiveram um resultado muito ruim com o conjunto extra de dados, mas foram bem com os dados de testes. Dos três modelos o ResNet152V2 teve a melhor acurácia, porém com o maior custo de tempo para treinamento, cerca de quase o dobro do tempo da DenseNet169 e quase o triplo da VGG16, que tem os menores tempos de treinamento e seis vezes menos tempo para predição.\n\nEm relação ao uso de GPU, todos os treinamentos utilizaram mais de 90% dos recursos da GPU.\n\n![](https://github.com/asantos2000/ml-ipt-hw/raw/master/pics/uso-gpu.jpg)","metadata":{}},{"cell_type":"code","source":"df_model_metrics","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:06:47.926780Z","iopub.execute_input":"2022-11-19T23:06:47.927181Z","iopub.status.idle":"2022-11-19T23:06:47.972975Z","shell.execute_reply.started":"2022-11-19T23:06:47.927147Z","shell.execute_reply":"2022-11-19T23:06:47.971727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hardware\n\nOs testes foram realizados no kaggle.com com a seguinte configuração:\n\n- Aceleração: GPU T4 x 2\n- Language python 3.7.10\n\nO Kaggle executa os notebooks em [container docker](https://github.com/Kaggle/docker-python).\n\nEm resumo o Kaggle disponibiliza o seguinte hardware:\n\n| Hardware Component          | Release Year | Core Count      | Memory | Hours/Week |\n| --------------------------- | ------------ | --------------- | ------ | ---------- |\n| Intel Xeon CPU 2.00 GHz CPU | 2012         | 4 vCPU cores    | 18 GB  | Unlimited  |\n| NVIDIA T4 (x2)              | 2018         | 2560 Cuda cores | 16 GB  | 33 h       | \n\nMais informações sobre o hardware.\n\n### CPU","metadata":{}},{"cell_type":"code","source":"!lscpu | grep -vE \"Vulnerability\"","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:06:54.891793Z","iopub.execute_input":"2022-11-19T23:06:54.892223Z","iopub.status.idle":"2022-11-19T23:06:56.313282Z","shell.execute_reply.started":"2022-11-19T23:06:54.892191Z","shell.execute_reply":"2022-11-19T23:06:56.312042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GPU","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-11-19T23:07:00.284185Z","iopub.execute_input":"2022-11-19T23:07:00.284614Z","iopub.status.idle":"2022-11-19T23:07:01.554604Z","shell.execute_reply.started":"2022-11-19T23:07:00.284581Z","shell.execute_reply":"2022-11-19T23:07:01.553430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Referências\n\n[^1]: ZHENG, Alice; CASARI, Amanda. Feature engineering for machine learning: principles and techniques for data scientists. \" O'Reilly Media, Inc.\", 2018.\n\nHUANG, Gao et al. Densely connected convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2017. p. 4700-4708.\n\nHE, Kaiming et al. Identity mappings in deep residual networks. In: European conference on computer vision. Springer, Cham, 2016. p. 630-645.\n\nSIMONYAN, Karen; ZISSERMAN, Andrew. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\n","metadata":{}}]}