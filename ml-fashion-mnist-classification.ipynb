{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Classificação de artigos de moda com ML e Fashion MNIST\n","\n","![](https://github.com/asantos2000/ml-ipt-hw/raw/master/pics/fashion_mnist_dataset_sample.png)\n","\n","Figure 1. Fashion-MNIST samples (by Zalando, MIT License)\n","\n","## 1. Problema\n","\n","> Em 2021, as receitas de comércio eletrônico de varejo das vendas de vestuário e acessórios nos Estados Unidos totalizaram 180,5 bilhões de dólares, aumentando de 144,8 bilhões em 2020. - [Stadist.com](https://www.statista.com/statistics/278890/us-apparel-and-accessories-retail-e-commerce-revenue/)\n","\n","Um grande problema que este mercado enfrenta é categorizar essas roupas e acessórios apenas pelas imagens, especialmente quando as categorias fornecidas pelas marcas são inconsistentes.\n","\n","Os clientes não reconhecem as categorias dos produtos que estão buscando e desistem após a primeira pesquisa ou navegação pelas categorias. Cerca de 15% dos usuários desistem na funcionalidade de busca e 30% quando usam o seletor de categorias. Com o uso de reconhecimento por imagem, deseja-se reduzir em 10% o número de desistências em seis meses.\n","\n","No armazém, estoquistas atribuem 15% das peças a categorias incorretas. Deseja-se reduzir em 10% os erros de atribuição.\n","\n","Deseja-se identificar, a partir de uma imagem provida pelo usuário, em um dispositivo móvel, a qual categoria aquele item pertence.\n","\n","### 1.1 Problema de ML\n","\n","O objetivo é classificar as imagens fornecidas em dez categorias (classificação múltipla) utilizando para treinamento o dataset Fashion MNist e obter uma taxa de acerto acima de 90%.\n","\n","Os seguintes modelos serão avaliados:\n","\n","- Aprendizado por transferência: Com os modelos ResNet152 V2, VGG-16, DenseNet169;\n","- SVM\n","- ConvNet\n","\n","Este trabalho está organizado da seguinte forma: 1. Discussão sobre o problema, 2. Planejamento do experimento, 3. Execução do experimento, 4. Análise e interpretação e Apresentação dos resultados."]},{"cell_type":"markdown","metadata":{},"source":["## 2. Condução do experimento\n","\n","### 2.1 Pipeline\n","\n","![](https://github.com/asantos2000/ml-ipt-hw/raw/master/pics/ml-pipeline.png)"]},{"cell_type":"markdown","metadata":{},"source":["## Bibliotecas"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2022-11-06T22:00:45.985333Z","iopub.status.busy":"2022-11-06T22:00:45.984963Z","iopub.status.idle":"2022-11-06T22:00:58.735597Z","shell.execute_reply":"2022-11-06T22:00:58.734798Z","shell.execute_reply.started":"2022-11-06T22:00:45.985247Z"},"trusted":true},"outputs":[],"source":["# Import important libraries\n","import time\n","import wandb\n","import os\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","from mod_util import *"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Globais\n","df_model_metrics = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Coleta de métrias de hardware\n","os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"ml-fashion-mnist-classification.ipynb\"\n","#os.environ[\"WANDB_API_KEY\"] = \"key\"\n","wandb.init(project=\"transfer-learning\")"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2 Descrição do conjunto de dados\n","\n","Fashion-MNIST é um conjunto de dados de imagens de artigos de Zalando—consistindo em um conjunto de treinamento de 60.000 exemplos e um conjunto de teste de 10.000 amostras. Cada amostra é uma imagem em tons de cinza 28x28, associada a um rótulo de 10 classes. Zalando pretende que o Fashion-MNIST sirva como um substituto direto para o conjunto de dados MNIST original para benchmarking de algoritmos de aprendizado de máquina.\n","\n","Ele compartilha o tamanho exato da imagem e a estrutura das divisões de treinamento e teste. O conjunto de dados MNIST original contém muitos dígitos manuscritos. Os membros da comunidade AI/ML/Data Science adoram esse conjunto de dados e o usam como referência para validar seus algoritmos. Na verdade, o MNIST é frequentemente o primeiro conjunto de dados que os pesquisadores tentam.\n","\n","Cada imagem tem 28 pixels de altura e 28 pixels de largura, para um total de 784 pixels no total.\n","Cada pixel tem um único valor de pixel associado a ele, indicando a claridade ou escuridão daquele pixel, com números mais altos significando mais escuro. Este valor de pixel é um número inteiro entre 0 e 255.\n","\n","Os conjuntos de dados de treinamento e teste têm 785 colunas. A primeira coluna é composta pelos rótulos das classes (veja acima), e representa a peça de vestuário. O restante das colunas contém os valores de pixel da imagem associada.\n","\n","Para localizar um pixel na imagem, suponha que decompusemos x como x = i * 28 + j, onde i e j são inteiros entre 0 e 27. O pixel está localizado na linha i e coluna j de uma matriz 28 x 28 . Por exemplo, pixel31 indica o pixel que está na quarta coluna da esquerda e na segunda linha da parte superior, como no diagrama ascii abaixo."]},{"cell_type":"markdown","metadata":{},"source":["## 3. Execução do experimento\n","\n","A execução do experimento está dividida em etapas:\n","\n","3.1 Preparar o conjunto de dados padrão;\n","3.2 Engenharia de requisitos;\n","3.3 Selecionar e treinar os modelos;\n","3.4 Avaliar os modelos;\n","3.5 Ajustar os modelos.\n","\n","### 3.1 Preparando o conjunto de dados\n","\n","Os modelos SVM e ConvNet não necessitam de modificações nos dados, podendo ser treinado com as imagens de tamanho 28x28 e um canal, porém, para transferência de conhecimento é necessário ajustá-las para no mínimo 48x48 com três canais (RGB).\n","\n","#### 3.1.1 Coletando e rotulando os dados\n","\n","O conjunto de dados Fashion do MNist já está rotulado e faz parte da biblioteca de conjuntos de dados do Keras."]},{"cell_type":"markdown","metadata":{},"source":["Formato dos conjuntos de dados:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:01:37.559611Z","iopub.status.busy":"2022-11-06T22:01:37.558831Z","iopub.status.idle":"2022-11-06T22:01:38.857120Z","shell.execute_reply":"2022-11-06T22:01:38.856388Z","shell.execute_reply.started":"2022-11-06T22:01:37.559568Z"},"trusted":true},"outputs":[],"source":["train_X, train_Y, test_X, test_Y = load_mnist_dataset()\n","\n","train_X.shape, train_Y.shape, test_X.shape, test_Y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:04:16.386442Z","iopub.status.busy":"2022-11-06T22:04:16.385594Z","iopub.status.idle":"2022-11-06T22:04:17.248778Z","shell.execute_reply":"2022-11-06T22:04:17.248052Z","shell.execute_reply.started":"2022-11-06T22:04:16.386401Z"},"trusted":true},"outputs":[],"source":["test_ext_X, test_ext_Y = load_extra_dataset(\"input/test_images\")\n","\n","test_ext_X.shape, test_ext_Y.shape"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.1.2 Avaliando os dados"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:02:07.883688Z","iopub.status.busy":"2022-11-06T22:02:07.883108Z","iopub.status.idle":"2022-11-06T22:02:07.891140Z","shell.execute_reply":"2022-11-06T22:02:07.889356Z","shell.execute_reply.started":"2022-11-06T22:02:07.883648Z"},"trusted":true},"outputs":[],"source":["dataset_size = train_X.shape[0]+test_X.shape[0]\n","train_size = train_X.shape[0]\n","test_size = test_X.shape[0]\n","extra_test_size = test_ext_X.shape[0]\n","\n","print(f\"Train with {train_size:,} images that represents {round(train_size / dataset_size*100,2)}% of dataset size of {dataset_size:,}.\")\n","print(f\"Test with {test_size:,} images that represents {round(test_size / dataset_size*100,2)}% of dataset size of {dataset_size:,}.\")\n","print(f\"Test with extra {extra_test_size:,} images that represents {round(extra_test_size / dataset_size*100,2)}% of dataset size of {dataset_size:,}.\")"]},{"cell_type":"markdown","metadata":{},"source":["Distribuição dos conjuntos de dados:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:04:26.475397Z","iopub.status.busy":"2022-11-06T22:04:26.475118Z","iopub.status.idle":"2022-11-06T22:04:26.518831Z","shell.execute_reply":"2022-11-06T22:04:26.517265Z","shell.execute_reply.started":"2022-11-06T22:04:26.475365Z"},"trusted":true},"outputs":[],"source":["train_data = pd.DataFrame(np.asarray(np.c_[train_Y, train_X.reshape(train_X.shape[0], 784)]))\n","test_data = pd.DataFrame(np.asarray(np.c_[test_Y, test_X.reshape(test_X.shape[0], 784)]))\n","test_ext_data = pd.DataFrame(np.asarray(np.c_[test_ext_Y, test_ext_X.reshape(test_ext_X.shape[0], 784)]))\n","\n","print(\"--- Train data ---\")\n","get_classes_distribution(train_data)\n","print(\"--- Test data ---\")\n","get_classes_distribution(test_data)\n","print(\"--- Test with extra data ---\")\n","get_classes_distribution(test_ext_data)"]},{"cell_type":"markdown","metadata":{},"source":["## 3.2 Engenharia de características \n","\n","Para o treinamento do modelo SVM foi aplicada a redução do número de caracteristicas utilizando HOG, já para os modelos pré treinados foi necessário aumentar esse conjunto, como demonstrado nas seções seguintes.\n","\n","### 3.2.1 Preprocessar os dados\n","\n","Para aplicar a transferência de conhecimento (_transfer learning_) para os modelos selecionado, é necessário:\n","\n","- Converter as imagens em 3 canais para ajustá-las a entrada dos modelos;\n","- Redefinir o formato para o formato do _tensor_ (requerido pelo tensorflow);\n","- Redimensionar as imagens para 48x48.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:04:37.314365Z","iopub.status.busy":"2022-11-06T22:04:37.314095Z","iopub.status.idle":"2022-11-06T22:04:39.190265Z","shell.execute_reply":"2022-11-06T22:04:39.189482Z","shell.execute_reply.started":"2022-11-06T22:04:37.314335Z"},"trusted":true},"outputs":[],"source":["train_X = adjust_data_for_transfer_learning(train_X, 48)\n","test_X = adjust_data_for_transfer_learning(test_X, 48)\n","test_ext_X = adjust_data_for_transfer_learning(test_ext_X, 48)\n","print(f\"train_X: {train_X.shape}\")\n","print(f\"test_X: {test_X.shape}\")\n","print(f\"test_ext_X: {test_ext_X.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["Formato e exemplo de uma figura no conjunto de dados."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:04:44.887433Z","iopub.status.busy":"2022-11-06T22:04:44.887165Z","iopub.status.idle":"2022-11-06T22:04:45.196353Z","shell.execute_reply":"2022-11-06T22:04:45.195645Z","shell.execute_reply.started":"2022-11-06T22:04:44.887403Z"},"trusted":true},"outputs":[],"source":["exibe_bitmap_primeira_imagem(train_X)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2.2 Normalizar os dados\n","\n","Os dados devem ser pré-processados antes de treinar a rede. Ao inspecionar a primeira imagem no conjunto de treinamento, verá que os valores de pixel ficam no intervalo de 0 a 255.\n","\n","Escalamos esses valores para um intervalo de 0 a 1 antes de alimentá-los ao modelo de rede neural. Para isso, dividimos os valores por 255. É importante que o conjunto de treinamento e o conjunto de teste sejam pré-processados da mesma forma."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:04:51.414107Z","iopub.status.busy":"2022-11-06T22:04:51.413825Z","iopub.status.idle":"2022-11-06T22:04:52.633235Z","shell.execute_reply":"2022-11-06T22:04:52.632464Z","shell.execute_reply.started":"2022-11-06T22:04:51.414074Z"},"trusted":true},"outputs":[],"source":["# Normalize the data and change data type\n","train_X = train_X / 255.\n","test_X = test_X / 255.\n","test_ext_X = test_ext_X / 255."]},{"cell_type":"markdown","metadata":{},"source":["Conjunto de dados normalizado:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:04:56.791860Z","iopub.status.busy":"2022-11-06T22:04:56.791563Z","iopub.status.idle":"2022-11-06T22:04:59.826192Z","shell.execute_reply":"2022-11-06T22:04:59.822234Z","shell.execute_reply.started":"2022-11-06T22:04:56.791824Z"},"trusted":true},"outputs":[],"source":["exibe_grade_imagens(AMOSTRAS_GRID, train_X, train_Y, must_reshape=False)"]},{"cell_type":"markdown","metadata":{},"source":["Conjunto extra de dados normalizado."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:05:12.539547Z","iopub.status.busy":"2022-11-06T22:05:12.538840Z","iopub.status.idle":"2022-11-06T22:05:16.242315Z","shell.execute_reply":"2022-11-06T22:05:16.241666Z","shell.execute_reply.started":"2022-11-06T22:05:12.539509Z"},"trusted":true},"outputs":[],"source":["exibe_grade_imagens(AMOSTRAS_GRID, test_ext_X, test_ext_Y, must_reshape=False)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2.3 Converter rótulos em codificador one-hot (para categórico)\n","\n","Para variáveis categóricas em que não existe relacionamento ordinal, a codificação inteira pode não ser suficiente, na melhor das hipóteses, ou enganosa para o modelo, na pior.\n","\n","Forçar uma relação ordinal por meio de uma codificação ordinal e permitir que o modelo assuma uma ordenação natural entre categorias pode resultar em desempenho ruim ou resultados inesperados.\n","\n","Nesse caso, uma codificação _one-hot_ pode ser aplicada à representação ordinal. É aqui que a variável codificada de inteiro é removida e uma nova variável binária é adicionada para cada valor inteiro exclusivo na variável.\n","\n","> Cada bit representa uma categoria possível. Se a variável não pode pertencer a várias categorias ao mesmo tempo, apenas um bit no grupo pode estar “ligado”. Isso é chamado de codificação one-hot [^1]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:05:22.999279Z","iopub.status.busy":"2022-11-06T22:05:22.999011Z","iopub.status.idle":"2022-11-06T22:05:23.005458Z","shell.execute_reply":"2022-11-06T22:05:23.004682Z","shell.execute_reply.started":"2022-11-06T22:05:22.999249Z"},"trusted":true},"outputs":[],"source":["train_Y_one_hot = to_categorical(train_Y)\n","test_Y_one_hot = to_categorical(test_Y)\n","test_ext_Y_one_hot = to_categorical(test_ext_Y) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:05:27.339558Z","iopub.status.busy":"2022-11-06T22:05:27.339280Z","iopub.status.idle":"2022-11-06T22:05:27.346286Z","shell.execute_reply":"2022-11-06T22:05:27.345282Z","shell.execute_reply.started":"2022-11-06T22:05:27.339526Z"},"trusted":true},"outputs":[],"source":["print(\"Examples:\")\n","print(f\"Category: {train_Y[0]}, Dummy vars: {train_Y_one_hot[0]}\")\n","print(f\"Category: {train_Y[1]}, Dummy vars: {train_Y_one_hot[1]}\")"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2.4 Dividindo os dados de treinamento em treinamento e validação"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:05:35.290535Z","iopub.status.busy":"2022-11-06T22:05:35.290083Z","iopub.status.idle":"2022-11-06T22:05:36.264398Z","shell.execute_reply":"2022-11-06T22:05:36.263625Z","shell.execute_reply.started":"2022-11-06T22:05:35.290498Z"},"trusted":true},"outputs":[],"source":["train_X, valid_X, train_label, valid_label = train_test_split(train_X,\n","                                                              train_Y_one_hot,\n","                                                              test_size=0.05,\n","                                                              random_state=42)"]},{"cell_type":"markdown","metadata":{},"source":["## 3.3 Selecionar e treinar os modelos\n","\n","Para transferência de aprendizado, foram escolhidos arbitrariamente três modelos com arquiteturas distintas do módulo de aplicativos do Keras. Eles são modelos de aprendizado profundo que são disponibilizados juntamente com pesos pré-treinados. Esses modelos podem ser usados para previsão, extração de recursos e ajuste fino.\n","\n","Com base na avaliação do keras dos [modelo disponíveis](https://keras.io/api/applications/), selecionamos modelos com acurária (Top-5) acima de 90%.\n","\n","Esses modelos são:\n","\n","1. [DenseNet169](https://keras.io/api/applications/densenet/#densenet169-function), uma rede convolucionais densamente conectadas (HUANG et al., 2017)\n","2. [ResNet152V2](https://keras.io/api/applications/resnet/#resnet152v2-function), uma redes residuais profundas. (RE et al., 2016)\n","3. e a [VGG-16](https://keras.io/api/applications/vgg/#vgg16-function), uma dede convolucional muito profundas para reconhecimento de imagem em grande escala (SIMONYAN et al., 2014)\n","\n","De acordo com essa [avaliação](https://keras.io/api/applications/), essas redes tem o seguinte desempenho:\n","\n","| Modelo      | Tamanho (MB) | Top-1 Acurácia | Top-5 Acurácia | Paâmetros  | Profundidade |\n","| ---         | ---          | ---            | ---            | ---        | ---          |\n","| DenseNet169 | 57           | 76.2%          | 93.2%          | 14.3M      | 338          |\n","| ResNet152V2 | 232          | 78.0%          | 94.2%          | 60.4M      | 307          |\n","| VGG16       | 528          | 71.3%          | 90.1%          | 138.4M     | 16           |\n","\n","> A profundidade conta o número de camadas com parâmetros.\n","\n","O DenseNet-169 foi escolhido porque, apesar de ter uma profundidade de 169 camadas, é relativamente baixo em parâmetros em comparação com outros modelos, e a arquitetura lida bem com o problema do gradiente de fuga.\n","\n","A escolha da ResNet é por causa da sua arquitetura, ela aprende com as funções residuais em vez de aprender com o sinal diretamente.\n","\n","A VGG16 é a escolha preferida da comunidade para extrair recursos de imagens. A configuração de peso do VGGNet está disponível publicamente e tem sido usada em muitos outros aplicativos e desafios como um extrator de recursos de linha de base. (SIMONYAN et al., 2014)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3.1 Definindo os modelos para tranferência de conhecimento (Pre-training models)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Hiperparametros"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:05:41.984217Z","iopub.status.busy":"2022-11-06T22:05:41.983921Z","iopub.status.idle":"2022-11-06T22:05:41.990655Z","shell.execute_reply":"2022-11-06T22:05:41.989756Z","shell.execute_reply.started":"2022-11-06T22:05:41.984173Z"},"trusted":true},"outputs":[],"source":["image_size = train_X[0].shape[0]\n","channels = 3\n","print(\"imageSize: \", image_size)\n","\n","epochs = 5\n","batch_size = 700"]},{"cell_type":"markdown","metadata":{},"source":["## 3.4 Treinar modelo"]},{"cell_type":"markdown","metadata":{},"source":["### DenseNet169"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:06:06.064620Z","iopub.status.busy":"2022-11-06T22:06:06.063731Z","iopub.status.idle":"2022-11-06T22:08:53.526486Z","shell.execute_reply":"2022-11-06T22:08:53.522361Z","shell.execute_reply.started":"2022-11-06T22:06:06.064567Z"},"trusted":true},"outputs":[],"source":["model_DenseNet169 = adj_model_DenseNet169(image_size, channels)\n","\n","# Train\n","start = time.perf_counter()\n","history_DenseNet169_model = model_DenseNet169.fit(train_X, \n","                                                train_label, \n","                                                validation_data = (valid_X,\n","                                                                   valid_label), \n","                                                epochs = epochs, \n","                                                batch_size = batch_size, \n","                                                verbose = 1)\n","end = time.perf_counter()\n","train_duration_DenseNet169 = end - start"]},{"cell_type":"markdown","metadata":{},"source":["Visualizando o modelo:"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-11-06T22:08:53.528681Z","iopub.status.busy":"2022-11-06T22:08:53.528221Z","iopub.status.idle":"2022-11-06T22:08:59.808913Z","shell.execute_reply":"2022-11-06T22:08:59.807803Z","shell.execute_reply.started":"2022-11-06T22:08:53.528640Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["plot_model(model_DenseNet169, \n","           to_file = \"plot-densenet169.png\", \n","           show_shapes = True, \n","           show_layer_names = True)"]},{"cell_type":"markdown","metadata":{},"source":["### ResNet152V2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:08:59.811566Z","iopub.status.busy":"2022-11-06T22:08:59.810994Z","iopub.status.idle":"2022-11-06T22:13:41.651196Z","shell.execute_reply":"2022-11-06T22:13:41.650226Z","shell.execute_reply.started":"2022-11-06T22:08:59.811510Z"},"trusted":true},"outputs":[],"source":["\n","model_ResNet152V2 = adj_model_ResNet152V2(image_size, channels)\n","\n","# Train\n","start = time.perf_counter()\n","history_ResNet152V2_model = model_ResNet152V2.fit(train_X, \n","                                                train_label, \n","                                                validation_data = (valid_X,\n","                                                                    valid_label), \n","                                                epochs = epochs, \n","                                                batch_size = batch_size, \n","                                                verbose = 1)\n","end = time.perf_counter()\n","train_duration_ResNet152V2 = end - start"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-11-06T22:13:41.654334Z","iopub.status.busy":"2022-11-06T22:13:41.653868Z","iopub.status.idle":"2022-11-06T22:13:45.955110Z","shell.execute_reply":"2022-11-06T22:13:45.945884Z","shell.execute_reply.started":"2022-11-06T22:13:41.654283Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["plot_model(model_ResNet152V2,\n","           to_file = \"plot-resnet152v2.png\", \n","           show_shapes = True, \n","           show_layer_names = True)"]},{"cell_type":"markdown","metadata":{},"source":["### VGG-16"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_VGG16 = adj_model_VGG16(image_size, channels)\n","\n","# Train\n","start = time.perf_counter()\n","history_VGG16_model = model_VGG16.fit(train_X, \n","                                      train_label, \n","                                      validation_data = (valid_X,\n","                                                        valid_label), \n","                                      epochs = epochs,\n","                                      batch_size = batch_size, \n","                                      verbose = 1)\n","end = time.perf_counter()\n","train_duration_VGG16 = end - start"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-11-06T22:15:35.869277Z","iopub.status.busy":"2022-11-06T22:15:35.869020Z","iopub.status.idle":"2022-11-06T22:15:36.459535Z","shell.execute_reply":"2022-11-06T22:15:36.458711Z","shell.execute_reply.started":"2022-11-06T22:15:35.869242Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["plot_model(model_VGG16,\n","           to_file = \"plot-vgg16.png\",\n","           show_shapes = True,\n","           show_layer_names = True)"]},{"cell_type":"markdown","metadata":{},"source":["## 3.4 Avaliar os modelos\n","\n","A métrica de avaliação para os modelos será a precisão multiclasse.\n","\n","### 3.4.1 Visualizando a acurácia e as perdas "]},{"cell_type":"markdown","metadata":{},"source":["#### DenseNet169"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:15:36.479159Z","iopub.status.busy":"2022-11-06T22:15:36.478891Z","iopub.status.idle":"2022-11-06T22:15:38.466299Z","shell.execute_reply":"2022-11-06T22:15:38.465600Z","shell.execute_reply.started":"2022-11-06T22:15:36.479124Z"},"trusted":true},"outputs":[],"source":["plot_acc_loss(history_DenseNet169_model, \"DenseNet169\", epochs);"]},{"cell_type":"markdown","metadata":{},"source":["#### ResNet152V2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:15:38.467967Z","iopub.status.busy":"2022-11-06T22:15:38.467696Z","iopub.status.idle":"2022-11-06T22:15:38.523243Z","shell.execute_reply":"2022-11-06T22:15:38.522263Z","shell.execute_reply.started":"2022-11-06T22:15:38.467932Z"},"trusted":true},"outputs":[],"source":["plot_acc_loss(history_ResNet152V2_model, \"ResNet152V2\", epochs);"]},{"cell_type":"markdown","metadata":{},"source":["#### VGG16"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:15:38.527401Z","iopub.status.busy":"2022-11-06T22:15:38.526682Z","iopub.status.idle":"2022-11-06T22:15:38.580444Z","shell.execute_reply":"2022-11-06T22:15:38.579753Z","shell.execute_reply.started":"2022-11-06T22:15:38.527357Z"},"trusted":true},"outputs":[],"source":["plot_acc_loss(history_VGG16_model, \"VGG16\", epochs);"]},{"cell_type":"markdown","metadata":{},"source":["### 3.4.2 Testando os modelos"]},{"cell_type":"markdown","metadata":{},"source":["#### DenseNet169"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:15:38.582152Z","iopub.status.busy":"2022-11-06T22:15:38.581544Z","iopub.status.idle":"2022-11-06T22:15:59.806245Z","shell.execute_reply":"2022-11-06T22:15:59.805522Z","shell.execute_reply.started":"2022-11-06T22:15:38.582101Z"},"trusted":true},"outputs":[],"source":["print(\"Avaliando DenseNet169\")\n","print(model_DenseNet169.metrics_names)\n","me = model_DenseNet169.evaluate(test_X, test_Y_one_hot)\n","print(me)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:15:59.807903Z","iopub.status.busy":"2022-11-06T22:15:59.807546Z","iopub.status.idle":"2022-11-06T22:16:00.253543Z","shell.execute_reply":"2022-11-06T22:16:00.252648Z","shell.execute_reply.started":"2022-11-06T22:15:59.807865Z"},"trusted":true},"outputs":[],"source":["print(\"Avaliando DenseNet169\")\n","print(model_DenseNet169.metrics_names)\n","me =model_DenseNet169.evaluate(test_ext_X, test_ext_Y_one_hot)\n","print(me)"]},{"cell_type":"markdown","metadata":{},"source":[" #### ResNet152V2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:16:00.255301Z","iopub.status.busy":"2022-11-06T22:16:00.254963Z","iopub.status.idle":"2022-11-06T22:16:13.908662Z","shell.execute_reply":"2022-11-06T22:16:13.907652Z","shell.execute_reply.started":"2022-11-06T22:16:00.255263Z"},"trusted":true},"outputs":[],"source":["print(\"Avaliando ResNet152V2\")\n","print(model_ResNet152V2.metrics_names)\n","me = model_ResNet152V2.evaluate(test_X, test_Y_one_hot)\n","print(me)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:16:13.912145Z","iopub.status.busy":"2022-11-06T22:16:13.910165Z","iopub.status.idle":"2022-11-06T22:16:14.384718Z","shell.execute_reply":"2022-11-06T22:16:14.383814Z","shell.execute_reply.started":"2022-11-06T22:16:13.912112Z"},"trusted":true},"outputs":[],"source":["print(\"Avaliando ResNet152V2\")\n","print(model_ResNet152V2.metrics_names)\n","me = model_ResNet152V2.evaluate(test_ext_X, test_ext_Y_one_hot)\n","print(me)"]},{"cell_type":"markdown","metadata":{},"source":["#### VGG16"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:16:14.386543Z","iopub.status.busy":"2022-11-06T22:16:14.386196Z","iopub.status.idle":"2022-11-06T22:16:19.901074Z","shell.execute_reply":"2022-11-06T22:16:19.900078Z","shell.execute_reply.started":"2022-11-06T22:16:14.386504Z"},"trusted":true},"outputs":[],"source":["print(\"Avaliando VGG-16\")\n","print(model_VGG16.metrics_names)\n","me = model_VGG16.evaluate(test_X, test_Y_one_hot)\n","print(me)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:16:19.903099Z","iopub.status.busy":"2022-11-06T22:16:19.902770Z","iopub.status.idle":"2022-11-06T22:16:20.104332Z","shell.execute_reply":"2022-11-06T22:16:20.103627Z","shell.execute_reply.started":"2022-11-06T22:16:19.903058Z"},"trusted":true},"outputs":[],"source":["print(\"Avaliando VGG-16\")\n","print(model_VGG16.metrics_names)\n","me = model_VGG16.evaluate(test_ext_X, test_ext_Y_one_hot)\n","print(me)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.4.3 Predições com os modelos\n","\n","Avaliação das previsões em relação ao _Ground Truth_."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:16:20.113937Z","iopub.status.busy":"2022-11-06T22:16:20.113475Z","iopub.status.idle":"2022-11-06T22:16:32.297536Z","shell.execute_reply":"2022-11-06T22:16:32.296857Z","shell.execute_reply.started":"2022-11-06T22:16:20.113900Z"},"trusted":true},"outputs":[],"source":["# predict DenseNet169 Model\n","start = time.perf_counter()\n","pred_Y_DenseNet169 = model_DenseNet169.predict(test_X)\n","end = time.perf_counter()\n","\n","show_predict(pred_Y_DenseNet169, test_X, test_Y, \"DenseNet169\")\n","\n","predict_duration_DenseNet169 =  end - start"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:16:32.299584Z","iopub.status.busy":"2022-11-06T22:16:32.299052Z","iopub.status.idle":"2022-11-06T22:16:47.411342Z","shell.execute_reply":"2022-11-06T22:16:47.410606Z","shell.execute_reply.started":"2022-11-06T22:16:32.299543Z"},"trusted":true},"outputs":[],"source":["# predict ResNet152V2 Model\n","start = time.perf_counter()\n","pred_Y_ResNet152V2 = model_ResNet152V2.predict(test_X)\n","end = time.perf_counter()\n","\n","show_predict(pred_Y_ResNet152V2, test_X, test_Y, \"ResNet152V2\")\n","\n","predict_duration_ResNet152V2 = predict_duration_DenseNet169"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:16:47.413386Z","iopub.status.busy":"2022-11-06T22:16:47.412899Z","iopub.status.idle":"2022-11-06T22:16:52.852407Z","shell.execute_reply":"2022-11-06T22:16:52.851637Z","shell.execute_reply.started":"2022-11-06T22:16:47.413348Z"},"trusted":true},"outputs":[],"source":["# predict VGG16 Model\n","start = time.perf_counter()\n","pred_Y_VGG16 = model_VGG16.predict(test_X)\n","end = time.perf_counter()\n","\n","show_predict(pred_Y_VGG16, test_X, test_Y, \"VGG16\")\n","\n","predict_duration_VGG16 = end - start"]},{"cell_type":"markdown","metadata":{},"source":["Predição com conjunto extra de dados"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:16:52.854414Z","iopub.status.busy":"2022-11-06T22:16:52.853949Z","iopub.status.idle":"2022-11-06T22:16:54.307563Z","shell.execute_reply":"2022-11-06T22:16:54.306920Z","shell.execute_reply.started":"2022-11-06T22:16:52.854378Z"},"trusted":true},"outputs":[],"source":["#predict DenseNet169 Model\n","pred_ext_Y_DenseNet169 = model_DenseNet169.predict(test_ext_X)\n","show_predict(pred_Y_DenseNet169, test_ext_X, test_ext_Y, \"DenseNet169\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:19:20.050759Z","iopub.status.busy":"2022-11-06T22:19:20.050388Z","iopub.status.idle":"2022-11-06T22:19:21.686232Z","shell.execute_reply":"2022-11-06T22:19:21.684485Z","shell.execute_reply.started":"2022-11-06T22:19:20.050713Z"},"trusted":true},"outputs":[],"source":["#predict ResNet152V2 Model\n","pred_ext_Y_ResNet152V2 = model_ResNet152V2.predict(test_ext_X)\n","show_predict(pred_Y_ResNet152V2, test_ext_X, test_ext_Y, \"ResNet152V2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:19:31.325518Z","iopub.status.busy":"2022-11-06T22:19:31.325244Z","iopub.status.idle":"2022-11-06T22:19:32.680497Z","shell.execute_reply":"2022-11-06T22:19:32.679683Z","shell.execute_reply.started":"2022-11-06T22:19:31.325485Z"},"trusted":true},"outputs":[],"source":["#predict VGG16 Model\n","pred_ext_Y_VGG16 = model_VGG16.predict(test_ext_X)\n","show_predict(pred_Y_VGG16, test_ext_X, test_ext_Y, \"VGG16\")"]},{"cell_type":"markdown","metadata":{},"source":["### 3.4.4 Matriz de confusão para verificar a precisão"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:17:12.229375Z","iopub.status.busy":"2022-11-06T22:17:12.229055Z","iopub.status.idle":"2022-11-06T22:17:12.940772Z","shell.execute_reply":"2022-11-06T22:17:12.939965Z","shell.execute_reply.started":"2022-11-06T22:17:12.229340Z"},"trusted":true},"outputs":[],"source":["# confusion matrix for DenseNet169 Model\n","show_confusion_matrix(test_Y, pred_Y_DenseNet169, \"DenseNet169\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:17:12.942358Z","iopub.status.busy":"2022-11-06T22:17:12.942016Z","iopub.status.idle":"2022-11-06T22:17:13.911045Z","shell.execute_reply":"2022-11-06T22:17:13.910246Z","shell.execute_reply.started":"2022-11-06T22:17:12.942311Z"},"trusted":true},"outputs":[],"source":["# confusion matrix for ResNet152V2 Model\n","show_confusion_matrix(test_Y, pred_Y_ResNet152V2, \"ResNet152V2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:17:13.919922Z","iopub.status.busy":"2022-11-06T22:17:13.917832Z","iopub.status.idle":"2022-11-06T22:17:14.797357Z","shell.execute_reply":"2022-11-06T22:17:14.796658Z","shell.execute_reply.started":"2022-11-06T22:17:13.919877Z"},"trusted":true},"outputs":[],"source":["# confusion matrix for VGG16 Model\n","show_confusion_matrix(test_Y, pred_Y_VGG16, \"VGG16\")"]},{"cell_type":"markdown","metadata":{},"source":["Matriz de confusão com conjunto extra de dados."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:17:14.799096Z","iopub.status.busy":"2022-11-06T22:17:14.798763Z","iopub.status.idle":"2022-11-06T22:17:15.469761Z","shell.execute_reply":"2022-11-06T22:17:15.469025Z","shell.execute_reply.started":"2022-11-06T22:17:14.799053Z"},"trusted":true},"outputs":[],"source":["# confusion matrix for DenseNet169 Model\n","show_confusion_matrix(test_ext_Y, pred_ext_Y_DenseNet169, \"DenseNet169\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:19:51.782893Z","iopub.status.busy":"2022-11-06T22:19:51.782583Z","iopub.status.idle":"2022-11-06T22:19:52.464610Z","shell.execute_reply":"2022-11-06T22:19:52.463741Z","shell.execute_reply.started":"2022-11-06T22:19:51.782856Z"},"trusted":true},"outputs":[],"source":["# confusion matrix for ResNet152V2 Model\n","show_confusion_matrix(test_ext_Y, pred_ext_Y_ResNet152V2, \"ResNet152V2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:19:57.416345Z","iopub.status.busy":"2022-11-06T22:19:57.416034Z","iopub.status.idle":"2022-11-06T22:19:58.105970Z","shell.execute_reply":"2022-11-06T22:19:58.105201Z","shell.execute_reply.started":"2022-11-06T22:19:57.416310Z"},"trusted":true},"outputs":[],"source":["# confusion matrix for VGG16 Model\n","show_confusion_matrix(test_ext_Y, pred_ext_Y_VGG16, \"VGG16\")"]},{"cell_type":"markdown","metadata":{},"source":["### 3.4.5 Relatório de classificação"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:20:15.752039Z","iopub.status.busy":"2022-11-06T22:20:15.751709Z","iopub.status.idle":"2022-11-06T22:20:15.774915Z","shell.execute_reply":"2022-11-06T22:20:15.774042Z","shell.execute_reply.started":"2022-11-06T22:20:15.752005Z"},"trusted":true},"outputs":[],"source":["# Classification Report for DenseNet169 Model\n","cr = show_classification_report(test_Y, pred_Y_DenseNet169, CLASS_NAMES, \"DenseNet169\")\n","\n","# Prepare report\n","df_model_metrics = df_model_metrics.append(add_model_metrics(cr, train_duration_DenseNet169, predict_duration_DenseNet169, \"DenseNet169\", 0))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:20:08.925270Z","iopub.status.busy":"2022-11-06T22:20:08.924847Z","iopub.status.idle":"2022-11-06T22:20:08.952928Z","shell.execute_reply":"2022-11-06T22:20:08.952067Z","shell.execute_reply.started":"2022-11-06T22:20:08.925236Z"},"trusted":true},"outputs":[],"source":["# Classification Report for ResNet152V2 Model\n","cr = show_classification_report(test_Y, pred_Y_ResNet152V2, class_names, \"ResNet152V2\")\n","\n","# Prepare report\n","df_model_metrics = df_model_metrics.append(add_model_metrics(cr, train_duration_ResNet152V2, predict_duration_ResNet152V2, \"ResNet152V2\", 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:20:11.712422Z","iopub.status.busy":"2022-11-06T22:20:11.711544Z","iopub.status.idle":"2022-11-06T22:20:11.737585Z","shell.execute_reply":"2022-11-06T22:20:11.736651Z","shell.execute_reply.started":"2022-11-06T22:20:11.712377Z"},"trusted":true},"outputs":[],"source":["# Classification Report for DenseNet169 Model\n","cr = show_classification_report(test_Y, pred_Y_VGG16, CLASS_NAMES, \"VGG16\")\n","\n","# Prepare report\n","df_model_metrics = df_model_metrics.append(add_model_metrics(cr, train_duration_VGG16, predict_duration_VGG16, \"VGG16\", 2))"]},{"cell_type":"markdown","metadata":{},"source":["Relatório de classificação com conjunto extra de dados."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:20:20.284713Z","iopub.status.busy":"2022-11-06T22:20:20.283905Z","iopub.status.idle":"2022-11-06T22:20:20.296699Z","shell.execute_reply":"2022-11-06T22:20:20.295648Z","shell.execute_reply.started":"2022-11-06T22:20:20.284676Z"},"trusted":true},"outputs":[],"source":["# Classification Report for DenseNet169 Model\n","show_classification_report(test_ext_Y, pred_ext_Y_DenseNet169, CLASS_NAMES, \"DenseNet169\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Globais\n","df_model_metrics = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:20:22.922688Z","iopub.status.busy":"2022-11-06T22:20:22.921958Z","iopub.status.idle":"2022-11-06T22:20:22.934321Z","shell.execute_reply":"2022-11-06T22:20:22.933372Z","shell.execute_reply.started":"2022-11-06T22:20:22.922648Z"},"trusted":true},"outputs":[],"source":["# Classification Report for ResNet152V2 Model\n","show_classification_report(test_ext_Y, pred_ext_Y_ResNet152V2, CLASS_NAMES, \"ResNet152V2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T22:20:25.451511Z","iopub.status.busy":"2022-11-06T22:20:25.450776Z","iopub.status.idle":"2022-11-06T22:20:25.462729Z","shell.execute_reply":"2022-11-06T22:20:25.461774Z","shell.execute_reply.started":"2022-11-06T22:20:25.451473Z"},"trusted":true},"outputs":[],"source":["# Classification Report for DenseNet169 Model\n","show_classification_report(test_ext_Y, pred_ext_Y_VGG16, class_names, \"VGG16\")"]},{"cell_type":"markdown","metadata":{},"source":["## Conclusão\n","\n","| modelo      | acurácia | precisão | recall | f1  | \n","| ----------- | -------- | -------- | ------ | --- |\n","| ResNet152V2 |          |          |        |     |\n","| VGG-16      |          |          |        |     |\n","| DenseNet169 |          |          |        |     |\n","| SVM-HOG     |          |          |        |     |\n","| SVM         |          |          |        |     |\n","| ConvNet     |          |          |        |     |\n","\n","Tempo treinamento, tempo execução (de um dataset) e memória\n","\n","Descrever o hardware"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_model_metrics"]},{"cell_type":"markdown","metadata":{},"source":["## Referências\n","\n","[^1]: ZHENG, Alice; CASARI, Amanda. Feature engineering for machine learning: principles and techniques for data scientists. \" O'Reilly Media, Inc.\", 2018.\n","\n","HUANG, Gao et al. Densely connected convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2017. p. 4700-4708.\n","\n","HE, Kaiming et al. Identity mappings in deep residual networks. In: European conference on computer vision. Springer, Cham, 2016. p. 630-645.\n","\n","SIMONYAN, Karen; ZISSERMAN, Andrew. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"vscode":{"interpreter":{"hash":"fd47821a561b6afd385678fa5a64c513739c456475a70c8411f90c36fdc84b13"}}},"nbformat":4,"nbformat_minor":4}
